#!/usr/bin/env -S uv --quiet run --script
# /// script
# requires-python = ">=3.10"
# dependencies = [
#     "click>=8.1.0",
#     "rich>=13.0.0",
#     "pyyaml",
# ]
# ///

"""
Find duplicate files in an Obsidian vault.

This script identifies duplicate files in an Obsidian vault based on three possible criteria:
1. Same filename (ignoring directory path) AND same content
2. Same filename only (regardless of content)
3. Same filename AND similar content (fuzzy matching)
"""

import hashlib
import json
import os
import pathlib
import re
from collections.abc import Generator
from dataclasses import dataclass
from datetime import datetime
from difflib import SequenceMatcher
from itertools import combinations
from typing import NamedTuple, Optional

import click
from rich import box
from rich.console import Console
from rich.panel import Panel
from rich.progress import Progress
from rich.table import Table

console = Console()


class FilePair(NamedTuple):
    """A pair of files that match according to some criteria."""

    file1: pathlib.Path
    file2: pathlib.Path
    similarity: float = 1.0


@dataclass
class DuplicateGroup:
    """A group of files that are duplicates according to some criteria."""

    files: list[pathlib.Path]
    similarity_matrix: Optional[dict[tuple[pathlib.Path, pathlib.Path], float]] = None

    def __post_init__(self):
        if self.similarity_matrix is None:
            self.similarity_matrix = {}
            for file1, file2 in combinations(self.files, 2):
                self.similarity_matrix[(file1, file2)] = 1.0


def compute_file_hash(filepath: pathlib.Path) -> str:
    """Compute SHA-256 hash of a file."""
    sha256_hash = hashlib.sha256()
    with open(filepath, "rb") as f:
        for byte_block in iter(lambda: f.read(4096), b""):
            sha256_hash.update(byte_block)
    return sha256_hash.hexdigest()


def compute_content_similarity(file1: pathlib.Path, file2: pathlib.Path) -> float:
    """
    Compute similarity between the contents of two files using difflib.
    Returns a value between 0.0 (completely different) and 1.0 (identical).
    """
    try:
        with (
            open(file1, "r", encoding="utf-8") as f1,
            open(file2, "r", encoding="utf-8") as f2,
        ):
            content1 = f1.read()
            content2 = f2.read()
            return SequenceMatcher(None, content1, content2).ratio()
    except UnicodeDecodeError:
        # If files are binary or not UTF-8 encoded, fall back to comparing if they're exactly identical
        return 1.0 if compute_file_hash(file1) == compute_file_hash(file2) else 0.0


def get_ignore_patterns(vault_dir: pathlib.Path) -> list[str]:
    """Get ignore patterns from Obsidian configuration."""
    ignore_patterns = []

    # Check for userIgnoreFilters in app.json
    app_json_path = vault_dir / ".obsidian" / "app.json"
    if app_json_path.exists():
        try:
            with open(app_json_path, "r", encoding="utf-8") as f:
                config = json.load(f)
                if "userIgnoreFilters" in config:
                    ignore_patterns.extend(config["userIgnoreFilters"])
        except (json.JSONDecodeError, IOError) as e:
            console.print(f"[yellow]Warning:[/] Could not read {app_json_path}: {e}")

    return ignore_patterns


def find_markdown_files(
    vault_dir: pathlib.Path, ignore_patterns: list[str]
) -> Generator[pathlib.Path, None, None]:
    """
    Recursively find all markdown files in the vault directory,
    respecting Obsidian ignore conventions.
    """
    for root, dirs, files in os.walk(vault_dir):
        # Remove hidden directories from the list to process
        dirs[:] = [d for d in dirs if not d.startswith(".")]

        # Filter out directories that match ignore patterns
        dirs[:] = [
            d
            for d in dirs
            if not any(re.match(pattern, d) for pattern in ignore_patterns)
        ]

        for file in files:
            # Skip hidden files
            if file.startswith("."):
                continue

            # Skip files that match ignore patterns
            if any(re.match(pattern, file) for pattern in ignore_patterns):
                continue

            # Only process markdown files
            if not file.endswith((".md", ".markdown")):
                continue

            yield pathlib.Path(root) / file


def get_file_dates(filepath: pathlib.Path) -> tuple[str, str, float]:
    """Get the creation and modification dates of a file in a human-readable format.

    Returns:
        A tuple of (creation_date, modification_date, modification_timestamp)
    """
    try:
        creation_time = filepath.stat().st_ctime
        modification_time = filepath.stat().st_mtime

        creation_date = datetime.fromtimestamp(creation_time).strftime("%Y-%m-%d %H:%M")
        modification_date = datetime.fromtimestamp(modification_time).strftime(
            "%Y-%m-%d %H:%M"
        )

        return creation_date, modification_date, modification_time
    except Exception as e:
        return f"Error: {e}", f"Error: {e}", 0.0


def get_relative_path(file_path: pathlib.Path, base_dir: pathlib.Path) -> str:
    """Get a shortened relative path for display purposes."""
    try:
        return str(file_path.relative_to(base_dir))
    except ValueError:
        # If the file is not relative to the base directory, return the full path
        return str(file_path)


def find_duplicate_files_by_name(
    directory: pathlib.Path,
    check_content: bool = False,
    fuzzy_match: bool = False,
    similarity_threshold: float = 0.8,
    ignore_patterns: list[str] = None,
) -> list[DuplicateGroup]:
    """
    Find duplicate files based on their names and optionally content.

    Args:
        directory: The root directory to search in
        check_content: If True, only consider files duplicates if their content matches
        fuzzy_match: If True, consider files duplicates if their content is similar above threshold
        similarity_threshold: Threshold for considering files similar (0.0 to 1.0)
        ignore_patterns: List of regex patterns for files/dirs to ignore

    Returns:
        List of DuplicateGroup objects
    """
    if ignore_patterns is None:
        ignore_patterns = []

    # Dictionary to group files by their name
    files_by_name = {}

    # Find all markdown files and group them by name
    with Progress() as progress:
        task = progress.add_task("[green]Finding files...", total=None)
        file_count = 0

        for file_path in find_markdown_files(directory, ignore_patterns):
            file_count += 1
            if file_count % 50 == 0:
                progress.update(
                    task, description=f"[green]Found {file_count} markdown files..."
                )

            filename = file_path.name
            if filename not in files_by_name:
                files_by_name[filename] = []
            files_by_name[filename].append(file_path)

        progress.update(task, total=file_count, completed=file_count)

    console.print(f"Processed {len(files_by_name)} unique filenames")

    # Filter out filenames that only appear once
    duplicate_groups = []

    with Progress() as progress:
        potential_dupes = {
            name: paths for name, paths in files_by_name.items() if len(paths) >= 2
        }
        task = progress.add_task(
            "[yellow]Checking duplicates...", total=len(potential_dupes)
        )

        for filename, file_paths in potential_dupes.items():
            progress.update(
                task,
                advance=1,
                description=f"[yellow]Checking duplicates for: {filename}",
            )

            if not check_content:
                # Mode 2: Same filename only
                duplicate_groups.append(DuplicateGroup(files=file_paths))
            else:
                # For content checking, we need to compute hashes or similarity
                if fuzzy_match:
                    # Mode 3: Same filename and similar content
                    # We create a similarity matrix for each pair of files
                    similarity_matrix = {}
                    for file1, file2 in combinations(file_paths, 2):
                        similarity = compute_content_similarity(file1, file2)
                        similarity_matrix[(file1, file2)] = similarity

                    # Group files that are similar above the threshold
                    # Start with each file in its own group
                    groups = [{file} for file in file_paths]

                    # Merge groups if they have similar files
                    i = 0
                    while i < len(groups):
                        j = i + 1
                        while j < len(groups):
                            # Check if any pair of files between groups are similar
                            merge = False
                            for file1 in groups[i]:
                                for file2 in groups[j]:
                                    pair = (
                                        (file1, file2)
                                        if file1 < file2
                                        else (file2, file1)
                                    )
                                    if (
                                        similarity_matrix.get(pair, 0)
                                        >= similarity_threshold
                                    ):
                                        merge = True
                                        break
                                if merge:
                                    break

                            if merge:
                                # Merge groups
                                groups[i].update(groups[j])
                                groups.pop(j)
                            else:
                                j += 1
                        i += 1

                    # Add groups with more than one file
                    for group in groups:
                        if len(group) > 1:
                            group_list = list(group)
                            duplicate_groups.append(
                                DuplicateGroup(
                                    files=group_list,
                                    similarity_matrix={
                                        k: v
                                        for k, v in similarity_matrix.items()
                                        if k[0] in group and k[1] in group
                                    },
                                )
                            )
                else:
                    # Mode 1: Same filename and identical content
                    # Group files by their hash
                    files_by_hash = {}
                    for file_path in file_paths:
                        file_hash = compute_file_hash(file_path)
                        if file_hash not in files_by_hash:
                            files_by_hash[file_hash] = []
                        files_by_hash[file_hash].append(file_path)

                    # Add groups with more than one file
                    for hash_val, hash_file_paths in files_by_hash.items():
                        if len(hash_file_paths) > 1:
                            duplicate_groups.append(
                                DuplicateGroup(files=hash_file_paths)
                            )

    return duplicate_groups


def display_duplicate_groups(
    duplicate_groups: list[DuplicateGroup], base_dir: pathlib.Path
):
    """Display the duplicate groups in a nice format."""
    if not duplicate_groups:
        console.print("[green]No duplicate files found![/green]")
        return

    console.print(
        f"[bold yellow]Found {len(duplicate_groups)} groups of duplicate files:[/bold yellow]"
    )

    for i, group in enumerate(duplicate_groups, 1):
        table = Table(
            title=f"Group {i}: '{group.files[0].name}'",
            show_header=True,
            header_style="bold",
            box=box.SIMPLE,
            title_justify="left",
            expand=False,
        )
        table.add_column("Path", style="dim", no_wrap=False)
        table.add_column("Created", style="green", justify="right", no_wrap=True)
        table.add_column("Modified", style="blue", justify="right", no_wrap=True)

        # If we have similarity data, add that column
        has_similarity = group.similarity_matrix and len(group.similarity_matrix) > 0
        if has_similarity:
            table.add_column("Similarity", style="cyan", justify="right", width=12)

        # Find the newest file in the group
        newest_time = 0
        for file_path in group.files:
            _, _, mod_time = get_file_dates(file_path)
            newest_time = max(newest_time, mod_time)

        # Add rows to the table
        for file_path in sorted(group.files, key=lambda p: str(p)):
            creation_date, modification_date, mod_time = get_file_dates(file_path)
            rel_path = get_relative_path(file_path, base_dir)

            # Determine if this is the newest file
            is_newest = (
                abs(mod_time - newest_time) < 0.001
            )  # Allow for tiny differences

            # Style the path based on whether it's the newest file
            styled_path = f"[bold]{rel_path}[/bold]" if is_newest else rel_path

            # Style the modification date based on whether it's the newest file
            styled_mod_date = (
                f"[bold yellow]{modification_date}[/bold yellow]"
                if is_newest
                else modification_date
            )

            if has_similarity:
                # Create a summary of similarity with other files
                similarities = []
                for other_path in sorted(group.files, key=lambda p: str(p)):
                    if file_path != other_path:
                        pair = (
                            (file_path, other_path)
                            if file_path < other_path
                            else (other_path, file_path)
                        )
                        similarity = group.similarity_matrix.get(pair, 1.0)
                        similarities.append(f"{similarity:.2f}")

                # Just show the average similarity
                avg_similarity = (
                    sum(float(s) for s in similarities) / len(similarities)
                    if similarities
                    else 1.0
                )
                table.add_row(
                    styled_path, creation_date, styled_mod_date, f"{avg_similarity:.2f}"
                )
            else:
                table.add_row(styled_path, creation_date, styled_mod_date)

        console.print(table)
        console.print()


@click.command()
@click.argument(
    "directory",
    type=click.Path(
        exists=True, file_okay=False, dir_okay=True, path_type=pathlib.Path
    ),
)
@click.option(
    "--mode",
    type=click.Choice(["name-and-content", "name-only", "similar-content"]),
    default="name-and-content",
    help="Duplicate detection mode",
)
@click.option(
    "--similarity-threshold",
    type=float,
    default=0.8,
    help="Similarity threshold (0.0-1.0) for similar-content mode",
)
def main(directory: pathlib.Path, mode: str, similarity_threshold: float):
    """Find duplicate markdown files in an Obsidian vault.

    This tool only scans Markdown files (.md, .markdown) and respects Obsidian's ignore patterns.

    DIRECTORY is the path to the Obsidian vault directory.
    """
    console.print(
        Panel.fit(
            f"[bold blue]Finding duplicate markdown files in: [/bold blue][yellow]{directory}[/yellow]",
            title="Obsidian Duplicate Finder",
        )
    )

    # Set parameters based on mode
    check_content = mode != "name-only"
    fuzzy_match = mode == "similar-content"

    console.print(f"[bold]Mode:[/bold] {mode}")
    if fuzzy_match:
        console.print(f"[bold]Similarity threshold:[/bold] {similarity_threshold}")

    # Get ignore patterns from Obsidian configuration
    ignore_patterns = get_ignore_patterns(directory)
    if ignore_patterns:
        console.print(
            f"[bold]Using Obsidian ignore patterns:[/bold] {', '.join(ignore_patterns)}"
        )

    with console.status("[bold green]Scanning for duplicate files...[/bold green]"):
        duplicate_groups = find_duplicate_files_by_name(
            directory=directory,
            check_content=check_content,
            fuzzy_match=fuzzy_match,
            similarity_threshold=similarity_threshold,
            ignore_patterns=ignore_patterns,
        )

    display_duplicate_groups(duplicate_groups, directory)

    # Print summary
    total_duplicates = sum(len(group.files) for group in duplicate_groups)
    if duplicate_groups:
        console.print(
            f"[bold green]Found {len(duplicate_groups)} groups with a total of {total_duplicates} files.[/bold green]"
        )


if __name__ == "__main__":
    main()
